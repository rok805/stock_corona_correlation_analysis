{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회사별 주식 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stock_crawling():\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.table_number = \"https://finance.naver.com/item/sise_day.nhn?code=\" # html안잡힘.\n",
    "        print('init')\n",
    "\n",
    "        \n",
    "        \n",
    "    def start(self): # 브라우저 실행.\n",
    "        driver = webdriver.Chrome(executable_path='chromedriver.exe') # 브라우저 열기\n",
    "        # 현재 디렉토리에 크롬드라이버 exe 파일 있음.\n",
    "        \n",
    "        driver.implicitly_wait(3) # 암묵적으로 웹 자원을 (최대) 3초 기다리기\n",
    "        \n",
    "        \n",
    "    def company(self, url, p_num):   # 회사별 주식 페이지 url 수집.\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path='chromedriver.exe') # 브라우저 열기\n",
    "\n",
    "        self.company_url=[]\n",
    "\n",
    "        for i in tqdm(range(1,p_num+1)):\n",
    "            \n",
    "            driver.get(str(url)+str(i)) # 시가총액 첫페이지\n",
    "            \n",
    "            time.sleep(1) # 웹페이지 열릴 시간을 줌.\n",
    "\n",
    "            html=driver.page_source # html 소스 \n",
    "            soup=BeautifulSoup(html, 'html.parser') # html 소스 정리\n",
    "\n",
    "            notices = soup.select('tbody')[1].find_all('a') # 페이지의 모든 회사 url 획득\n",
    "\n",
    "            for j in notices:\n",
    "                if 'main' in j['href']:\n",
    "                    self.company_url.append('https://finance.naver.com/'+str(j['href']))\n",
    "\n",
    "\n",
    "        return self.company_url\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def stock(self,t_num,c_url): # t_num: 시세 테이블 페이지 수\n",
    "\n",
    "        import pandas as pd\n",
    "        \n",
    "        self.t_num=t_num\n",
    "        self.company_url=c_url\n",
    "        print(self.company_url)\n",
    "        \n",
    "        #시세 table number\n",
    "        self.company_price=pd.DataFrame()\n",
    "\n",
    "        driver = webdriver.Chrome(executable_path='chromedriver.exe') # 브라우저 열기\n",
    "\n",
    "        \n",
    "        for c_url in tqdm(self.company_url):\n",
    "            data=pd.DataFrame() # 회사마다 시세가 잠시 담기는 곳.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            driver.get(c_url)  # 회사 페이지 진입\n",
    "            time.sleep(0.1)\n",
    "            html=driver.page_source\n",
    "            soup=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "\n",
    "            # 회사 이름\n",
    "            c_name = soup.select('#middle > dl.blind > dd ')[1].text.split()[1:] # 이름 획득\n",
    "            c_name = \" \".join(c_name)\n",
    "            print(c_name)\n",
    "\n",
    "            # 업종\n",
    "            try:\n",
    "                d_name = soup.select('#content > div > h4 > em > a')[0].text # 이름 획득\n",
    "                print(d_name)\n",
    "            except:\n",
    "                print('업종없음')\n",
    "                d_name = '없종없음'\n",
    "\n",
    "\n",
    "            price_url = soup.select('#content > ul > li > a.tab2')[0]['href'] \n",
    "            price_url = \"https://finance.naver.com/\"+price_url\n",
    "            driver.get(price_url) # 시세 페이지 진입\n",
    "            print(price_url)\n",
    "\n",
    "\n",
    "            for num in range(1,self.t_num+1):\n",
    "                table_url = self.table_number+c_url[-6:]+\"&page=\"+str(num) # 고정 url, 지정 변수 입력(self.table_number)\n",
    "\n",
    "                driver.get(table_url)\n",
    "                time.sleep(0.1)\n",
    "                data = pd.concat([data,pd.read_html(table_url, encoding='cp949')[0]]) # 시세 테이블 획득.\n",
    "\n",
    "            data['회사이름'] = [c_name for _ in range(len(data))]\n",
    "            data['업종'] = [d_name for _ in range(len(data))]\n",
    "            self.company_price = pd.concat([self.company_price,data], ignore_index=True)\n",
    "\n",
    "        return self.company_price\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def run(self,pages=12): # 위의 함수들을 실행시킬 함수.\n",
    "        \n",
    "        \n",
    "        self.pages = pages\n",
    "\n",
    "        play=stock_crawling() #1. 초기화\n",
    "        \n",
    "        play.start() #2. 브라우저 실행\n",
    "\n",
    "        \n",
    "        \n",
    "        self.url='https://finance.naver.com/sise/sise_market_sum.nhn?&page=' #고정 url, 지정변수 입력\n",
    "        p_num=32\n",
    "        self.c_url = play.company(self.url,p_num) #3. 회사별 주식 페이지 url 수집\n",
    "\n",
    "        stock_price = play.stock(self.pages, self.c_url) #4. 회사별 주식 수집, pages: 시세 테이블 페이지 수\n",
    "\n",
    "        return stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=stock_crawling()\n",
    "\n",
    "stock_price = start.run(12) #  회사별 종가 테이블 12페이지 까지만 가져옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price.to_excel(r'C:\\Users\\user\\증권크롤링\\주식 시세.xlsx') # 크롤링 결과 엑셀 파일로 저장."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
